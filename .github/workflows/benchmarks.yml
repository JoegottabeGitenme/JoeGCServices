name: Benchmarks

on:
  push:
    branches: [main]
    paths:
      - 'crates/renderer/**'
      - 'crates/grib2-parser/**'
      - 'crates/projection/**'
      - 'crates/wms-common/**'
  pull_request:
    paths:
      - 'crates/renderer/**'
      - 'crates/grib2-parser/**'
      - 'crates/projection/**'
      - 'crates/wms-common/**'
  workflow_dispatch:
    inputs:
      run_all:
        description: 'Run all benchmarks regardless of changes'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pull-requests: write
  deployments: write

env:
  CARGO_TERM_COLOR: always

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      renderer: ${{ steps.filter.outputs.renderer }}
      grib2-parser: ${{ steps.filter.outputs.grib2-parser }}
      projection: ${{ steps.filter.outputs.projection }}
      wms-common: ${{ steps.filter.outputs.wms-common }}
      any_changed: ${{ steps.filter.outputs.any_changed }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changed crates
        id: filter
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ inputs.run_all }}" == "true" ]]; then
            echo "renderer=true" >> $GITHUB_OUTPUT
            echo "grib2-parser=true" >> $GITHUB_OUTPUT
            echo "projection=true" >> $GITHUB_OUTPUT
            echo "wms-common=true" >> $GITHUB_OUTPUT
            echo "any_changed=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # For PRs, compare against base branch; for pushes, compare against previous commit
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            BASE_SHA=${{ github.event.pull_request.base.sha }}
          else
            BASE_SHA=${{ github.event.before }}
          fi

          # Get changed files
          CHANGED_FILES=$(git diff --name-only $BASE_SHA ${{ github.sha }} 2>/dev/null || echo "")
          
          echo "Changed files:"
          echo "$CHANGED_FILES"

          # Check each crate
          if echo "$CHANGED_FILES" | grep -q "^crates/renderer/"; then
            echo "renderer=true" >> $GITHUB_OUTPUT
          else
            echo "renderer=false" >> $GITHUB_OUTPUT
          fi

          if echo "$CHANGED_FILES" | grep -q "^crates/grib2-parser/"; then
            echo "grib2-parser=true" >> $GITHUB_OUTPUT
          else
            echo "grib2-parser=false" >> $GITHUB_OUTPUT
          fi

          if echo "$CHANGED_FILES" | grep -q "^crates/projection/"; then
            echo "projection=true" >> $GITHUB_OUTPUT
          else
            echo "projection=false" >> $GITHUB_OUTPUT
          fi

          if echo "$CHANGED_FILES" | grep -q "^crates/wms-common/"; then
            echo "wms-common=true" >> $GITHUB_OUTPUT
          else
            echo "wms-common=false" >> $GITHUB_OUTPUT
          fi

          # Check if any benchmarkable crate changed
          if echo "$CHANGED_FILES" | grep -qE "^crates/(renderer|grib2-parser|projection|wms-common)/"; then
            echo "any_changed=true" >> $GITHUB_OUTPUT
          else
            echo "any_changed=false" >> $GITHUB_OUTPUT
          fi

  benchmark:
    needs: detect-changes
    if: needs.detect-changes.outputs.any_changed == 'true'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-
            ${{ runner.os }}-cargo-

      - name: Run renderer benchmarks
        if: needs.detect-changes.outputs.renderer == 'true' || needs.detect-changes.outputs.wms-common == 'true' || needs.detect-changes.outputs.projection == 'true'
        run: |
          # Run benchmarks with JSON output for github-action-benchmark
          cargo bench --package renderer -- --noplot --save-baseline current 2>&1 | tee renderer_bench.txt
          
          # Also output in a format suitable for parsing
          # Criterion outputs JSON to target/criterion/<benchmark>/new/estimates.json

      - name: Convert Criterion output to benchmark format
        id: convert
        run: |
          # Create a JSON file in the format expected by github-action-benchmark
          # We'll parse Criterion's estimates.json files
          
          cat > parse_criterion.py << 'PYTHON_SCRIPT'
          import json
          import os
          import sys
          
          results = []
          criterion_dir = "target/criterion"
          
          def find_estimates(path):
              """Recursively find all estimates.json files"""
              for root, dirs, files in os.walk(path):
                  if "estimates.json" in files and "new" in root:
                      yield os.path.join(root, "estimates.json")
          
          def get_benchmark_name(path):
              """Extract benchmark name from path"""
              # Path like: target/criterion/goes_pipeline/ir_tile_256x256/new/estimates.json
              parts = path.replace(criterion_dir + "/", "").split("/")
              # Remove 'new' and 'estimates.json'
              name_parts = [p for p in parts if p not in ["new", "estimates.json"]]
              return "/".join(name_parts)
          
          if os.path.exists(criterion_dir):
              for est_file in find_estimates(criterion_dir):
                  try:
                      with open(est_file) as f:
                          data = json.load(f)
                      
                      name = get_benchmark_name(est_file)
                      mean_ns = data.get("mean", {}).get("point_estimate", 0)
                      
                      # Convert to appropriate unit
                      if mean_ns > 0:
                          results.append({
                              "name": name,
                              "unit": "ns",
                              "value": mean_ns,
                              "range": f"Â± {data.get('mean', {}).get('standard_error', 0):.2f}"
                          })
                  except Exception as e:
                      print(f"Warning: Could not parse {est_file}: {e}", file=sys.stderr)
          
          # Sort by name for consistent output
          results.sort(key=lambda x: x["name"])
          
          print(json.dumps(results, indent=2))
          PYTHON_SCRIPT
          
          python3 parse_criterion.py > benchmark_results.json
          
          echo "Benchmark results:"
          cat benchmark_results.json

      - name: Download previous benchmark data from gh-pages
        id: download-baseline
        run: |
          # Try to fetch the benchmark data from gh-pages
          BENCHMARK_URL="https://raw.githubusercontent.com/${{ github.repository }}/gh-pages/dev/bench/data.js"
          
          echo "Fetching baseline from: $BENCHMARK_URL"
          
          if curl -fsSL "$BENCHMARK_URL" -o baseline_data.js 2>/dev/null; then
            # Extract JSON from the JavaScript file (it's in format: window.BENCHMARK_DATA = {...})
            # The data.js file contains: window.BENCHMARK_DATA = { entries: { "Rust Benchmarks": [...] } }
            sed 's/^window\.BENCHMARK_DATA = //' baseline_data.js | sed 's/;$//' > baseline_data.json
            
            # Extract just the latest benchmark entry for comparison
            cat > extract_baseline.py << 'PYTHON_SCRIPT'
          import json
          import sys

          try:
              with open('baseline_data.json') as f:
                  data = json.load(f)
              
              # Get the entries for "Rust Benchmarks"
              entries = data.get('entries', {}).get('Rust Benchmarks', [])
              
              if entries:
                  # Get the most recent entry (last in the array)
                  latest = entries[-1]
                  benchmarks = latest.get('benches', [])
                  
                  # Create a dict for easy lookup: name -> value
                  baseline = {b['name']: b['value'] for b in benchmarks}
                  
                  with open('baseline_lookup.json', 'w') as f:
                      json.dump(baseline, f)
                  
                  print(f"Found {len(baseline)} baseline benchmarks from commit {latest.get('commit', {}).get('id', 'unknown')[:8]}")
                  sys.exit(0)
              else:
                  print("No baseline entries found")
                  sys.exit(1)
          except Exception as e:
              print(f"Error extracting baseline: {e}")
              sys.exit(1)
          PYTHON_SCRIPT
            
            if python3 extract_baseline.py; then
              echo "has_baseline=true" >> $GITHUB_OUTPUT
            else
              echo "has_baseline=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "No baseline data found (gh-pages may not exist yet)"
            echo "has_baseline=false" >> $GITHUB_OUTPUT
          fi

      - name: Create gh-pages branch if it doesn't exist
        if: github.ref == 'refs/heads/main'
        run: |
          # Check if gh-pages branch exists
          if ! git ls-remote --heads origin gh-pages | grep -q gh-pages; then
            echo "Creating gh-pages branch..."
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            
            # Create orphan branch with initial commit
            git checkout --orphan gh-pages
            git reset --hard
            git commit --allow-empty -m "Initialize gh-pages for benchmark data"
            git push origin gh-pages
            
            # Switch back to the original branch
            git checkout -f ${{ github.sha }}
            echo "gh-pages branch created successfully"
          else
            echo "gh-pages branch already exists"
          fi

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Rust Benchmarks
          tool: 'customSmallerIsBetter'
          output-file-path: benchmark_results.json
          # Store benchmark data in gh-pages branch
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          # Always push to gh-pages on main branch
          auto-push: ${{ github.ref == 'refs/heads/main' }}
          # Comment on PRs with comparison
          comment-on-alert: true
          comment-always: ${{ github.event_name == 'pull_request' }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          # Alert if performance regresses by more than 10%
          alert-threshold: '110%'
          fail-on-alert: false
          # Summary will be added to the workflow
          summary-always: true

      - name: Generate benchmark summary with comparison
        if: always()
        run: |
          echo "## ðŸ“Š Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Show which crates were benchmarked
          echo "### Crates Benchmarked" >> $GITHUB_STEP_SUMMARY
          echo "| Crate | Changed | Benchmarked |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|---------|-------------|" >> $GITHUB_STEP_SUMMARY
          
          RENDERER="${{ needs.detect-changes.outputs.renderer }}"
          GRIB2="${{ needs.detect-changes.outputs.grib2-parser }}"
          PROJECTION="${{ needs.detect-changes.outputs.projection }}"
          WMS_COMMON="${{ needs.detect-changes.outputs.wms-common }}"
          
          # renderer benchmarks run if renderer, wms-common, or projection changed
          if [[ "$RENDERER" == "true" || "$WMS_COMMON" == "true" || "$PROJECTION" == "true" ]]; then
            RENDERER_BENCHED="âœ…"
          else
            RENDERER_BENCHED="âŒ"
          fi
          
          echo "| renderer | $([[ "$RENDERER" == "true" ]] && echo "âœ…" || echo "âŒ") | $RENDERER_BENCHED |" >> $GITHUB_STEP_SUMMARY
          echo "| grib2-parser | $([[ "$GRIB2" == "true" ]] && echo "âœ…" || echo "âŒ") | $([[ "$GRIB2" == "true" ]] && echo "âœ…" || echo "âŒ") |" >> $GITHUB_STEP_SUMMARY
          echo "| projection | $([[ "$PROJECTION" == "true" ]] && echo "âœ…" || echo "âŒ") | $RENDERER_BENCHED |" >> $GITHUB_STEP_SUMMARY
          echo "| wms-common | $([[ "$WMS_COMMON" == "true" ]] && echo "âœ…" || echo "âŒ") | $RENDERER_BENCHED |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Show benchmark results with comparison to baseline
          if [ -f benchmark_results.json ]; then
            HAS_BASELINE="${{ steps.download-baseline.outputs.has_baseline }}"
            
            if [[ "$HAS_BASELINE" == "true" ]]; then
              echo "### Benchmark Results (vs Previous)" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "| Benchmark | Current | Previous | Change |" >> $GITHUB_STEP_SUMMARY
              echo "|-----------|---------|----------|--------|" >> $GITHUB_STEP_SUMMARY
            else
              echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "> â„¹ï¸ *No baseline data available yet. Comparison will be available after first run on main.*" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "| Benchmark | Time |" >> $GITHUB_STEP_SUMMARY
              echo "|-----------|------|" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Parse and format results with comparison
            cat > format_results.py << 'PYTHON_SCRIPT'
          import json
          import os

          def format_time(ns):
              if ns >= 1_000_000_000:
                  return f"{ns / 1_000_000_000:.2f} s"
              elif ns >= 1_000_000:
                  return f"{ns / 1_000_000:.2f} ms"
              elif ns >= 1_000:
                  return f"{ns / 1_000:.2f} Âµs"
              else:
                  return f"{ns:.0f} ns"

          def format_change(current, previous):
              if previous == 0:
                  return "N/A"
              pct = ((current - previous) / previous) * 100
              if pct < -5:
                  return f"ðŸŸ¢ {pct:+.1f}%"
              elif pct > 5:
                  return f"ðŸ”´ {pct:+.1f}%"
              elif pct > 0:
                  return f"ðŸŸ¡ {pct:+.1f}%"
              else:
                  return f"ðŸŸ¡ {pct:.1f}%"

          # Load current results
          with open('benchmark_results.json') as f:
              current_results = json.load(f)

          # Try to load baseline
          baseline = {}
          has_baseline = os.path.exists('baseline_lookup.json')
          if has_baseline:
              with open('baseline_lookup.json') as f:
                  baseline = json.load(f)

          # Output results
          for r in current_results[:25]:  # Limit to 25 rows
              name = r['name']
              current_ns = r['value']
              current_str = format_time(current_ns)
              
              if has_baseline:
                  prev_ns = baseline.get(name, 0)
                  if prev_ns > 0:
                      prev_str = format_time(prev_ns)
                      change_str = format_change(current_ns, prev_ns)
                  else:
                      prev_str = "â€”"
                      change_str = "ðŸ†• new"
                  print(f"| {name} | {current_str} | {prev_str} | {change_str} |")
              else:
                  print(f"| {name} | {current_str} |")

          if len(current_results) > 25:
              remaining = len(current_results) - 25
              if has_baseline:
                  print(f"| *... and {remaining} more* | | | |")
              else:
                  print(f"| *... and {remaining} more* | |")
          PYTHON_SCRIPT
            
            python3 format_results.py >> $GITHUB_STEP_SUMMARY
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "---" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ“ˆ **[View Full Benchmark History](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/dev/bench/)**" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "<details><summary>Legend</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸŸ¢ **Faster** (>5% improvement)" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸŸ¡ **Similar** (within Â±5%)" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”´ **Slower** (>5% regression)" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ†• **New** benchmark (no baseline)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Comment on PR with comparison
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            const formatTime = (ns) => {
              if (ns >= 1_000_000_000) return `${(ns / 1_000_000_000).toFixed(2)} s`;
              if (ns >= 1_000_000) return `${(ns / 1_000_000).toFixed(2)} ms`;
              if (ns >= 1_000) return `${(ns / 1_000).toFixed(2)} Âµs`;
              return `${ns.toFixed(0)} ns`;
            };
            
            const formatChange = (current, previous) => {
              if (previous === 0) return 'N/A';
              const pct = ((current - previous) / previous) * 100;
              if (pct < -5) return `ðŸŸ¢ ${pct.toFixed(1)}%`;
              if (pct > 5) return `ðŸ”´ +${pct.toFixed(1)}%`;
              return `ðŸŸ¡ ${pct >= 0 ? '+' : ''}${pct.toFixed(1)}%`;
            };
            
            let body = '## ðŸ“Š Benchmark Results\n\n';
            
            // Load current results
            let currentResults = [];
            if (fs.existsSync('benchmark_results.json')) {
              currentResults = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
            }
            
            // Load baseline if available
            let baseline = {};
            let hasBaseline = fs.existsSync('baseline_lookup.json');
            if (hasBaseline) {
              baseline = JSON.parse(fs.readFileSync('baseline_lookup.json', 'utf8'));
            }
            
            if (hasBaseline) {
              body += '| Benchmark | Current | Previous | Change |\n';
              body += '|-----------|---------|----------|--------|\n';
              
              currentResults.slice(0, 20).forEach(r => {
                const prevNs = baseline[r.name] || 0;
                const currentStr = formatTime(r.value);
                const prevStr = prevNs > 0 ? formatTime(prevNs) : 'â€”';
                const change = prevNs > 0 ? formatChange(r.value, prevNs) : 'ðŸ†• new';
                body += `| ${r.name} | ${currentStr} | ${prevStr} | ${change} |\n`;
              });
            } else {
              body += '> â„¹ï¸ *No baseline data available yet.*\n\n';
              body += '| Benchmark | Time |\n';
              body += '|-----------|------|\n';
              
              currentResults.slice(0, 20).forEach(r => {
                body += `| ${r.name} | ${formatTime(r.value)} |\n`;
              });
            }
            
            if (currentResults.length > 20) {
              body += `\n*... and ${currentResults.length - 20} more benchmarks*\n`;
            }
            
            body += '\n\n### Crates Benchmarked\n';
            body += '| Crate | Changed | Benchmarked |\n';
            body += '|-------|---------|-------------|\n';
            
            const renderer = '${{ needs.detect-changes.outputs.renderer }}' === 'true';
            const grib2 = '${{ needs.detect-changes.outputs.grib2-parser }}' === 'true';
            const projection = '${{ needs.detect-changes.outputs.projection }}' === 'true';
            const wmsCommon = '${{ needs.detect-changes.outputs.wms-common }}' === 'true';
            
            const rendererBenched = renderer || wmsCommon || projection;
            
            body += `| renderer | ${renderer ? 'âœ…' : 'âŒ'} | ${rendererBenched ? 'âœ…' : 'âŒ'} |\n`;
            body += `| grib2-parser | ${grib2 ? 'âœ…' : 'âŒ'} | ${grib2 ? 'âœ…' : 'âŒ'} |\n`;
            body += `| projection | ${projection ? 'âœ…' : 'âŒ'} | ${rendererBenched ? 'âœ…' : 'âŒ'} |\n`;
            body += `| wms-common | ${wmsCommon ? 'âœ…' : 'âŒ'} | ${rendererBenched ? 'âœ…' : 'âŒ'} |\n`;
            
            body += '\n\n> ðŸ“ˆ [View full benchmark history](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/dev/bench/)\n';
            body += '\n<details><summary>Legend</summary>\n\n';
            body += '- ðŸŸ¢ **Faster** (>5% improvement)\n';
            body += '- ðŸŸ¡ **Similar** (within Â±5%)\n';
            body += '- ðŸ”´ **Slower** (>5% regression)\n';
            body += '- ðŸ†• **New** benchmark (no baseline)\n';
            body += '\n</details>';
            
            // Find existing comment to update
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('## ðŸ“Š Benchmark Results')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

  # Summary job that always runs to report if benchmarks were skipped
  summary:
    needs: [detect-changes, benchmark]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Summary
        run: |
          echo "## ðŸ“Š Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.detect-changes.outputs.any_changed }}" != "true" ]]; then
            echo "â­ï¸ **No benchmark-relevant crates were changed. Benchmarks skipped.**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Benchmarks run when these paths change:" >> $GITHUB_STEP_SUMMARY
            echo "- \`crates/renderer/**\`" >> $GITHUB_STEP_SUMMARY
            echo "- \`crates/grib2-parser/**\`" >> $GITHUB_STEP_SUMMARY
            echo "- \`crates/projection/**\`" >> $GITHUB_STEP_SUMMARY
            echo "- \`crates/wms-common/**\`" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.benchmark.result }}" == "success" ]]; then
            echo "âœ… **Benchmarks completed successfully!**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Results have been:" >> $GITHUB_STEP_SUMMARY
            if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
              echo "- ðŸ“¤ Pushed to benchmark history (gh-pages branch)" >> $GITHUB_STEP_SUMMARY
            fi
            echo "- ðŸ“Š Added to the job summary above" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.benchmark.result }}" == "failure" ]]; then
            echo "âŒ **Benchmarks failed!** Check the job logs for details." >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Benchmark job status: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Changed Crates" >> $GITHUB_STEP_SUMMARY
          echo "| Crate | Changed |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| renderer | ${{ needs.detect-changes.outputs.renderer == 'true' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| grib2-parser | ${{ needs.detect-changes.outputs.grib2-parser == 'true' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| projection | ${{ needs.detect-changes.outputs.projection == 'true' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| wms-common | ${{ needs.detect-changes.outputs.wms-common == 'true' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“ˆ **[View Benchmark History](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/dev/bench/)**" >> $GITHUB_STEP_SUMMARY
